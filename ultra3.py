import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import json
import datetime
# âœ… Ø¥Ø¶Ø§ÙØ© timezone Ù‡Ù†Ø§ Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ­Ø°ÙŠØ±
from datetime import timedelta, timezone
import re
import sys
import time
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
# âœ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª Firebase
import firebase_admin
from firebase_admin import credentials, firestore
from flask import Flask 
from threading import Thread 
import os 

# ==========================================
# âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================
# âœ… Ø¬Ù„Ø¨ Ù…ÙØ§ØªÙŠØ­ Firebase Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
FIREBASE_CREDENTIALS_JSON = os.getenv("FIREBASE_CREDENTIALS")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", 60))

# âœ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Cloud Firestore
db = None
if FIREBASE_CREDENTIALS_JSON:
    try:
        cred_dict = json.loads(FIREBASE_CREDENTIALS_JSON)
        cred = credentials.Certificate(cred_dict)
        if not firebase_admin._apps:
            firebase_admin.initialize_app(cred)
        db = firestore.client()
        print("âœ… Cloud Firestore Initialized Successfully.")
    except Exception as e:
        print(f"âŒ Firestore Init Error: {e}")
else:
    print("âš ï¸ Warning: FIREBASE_CREDENTIALS is missing.")

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø³ÙŠØ±ÙØ± ÙˆÙ‡Ù…ÙŠ (Flask)
# ==========================================
app = Flask('')

@app.route('/')
def home():
    return "I am alive! The Bot is running with Firestore..."

def run():
    app.run(host='0.0.0.0', port=8080)

def keep_alive():
    t = Thread(target=run)
    t.start()

# ==========================================
# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ 
# ==========================================
BASE_URL = "https://www.ysscores.com"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Referer': 'https://www.google.dz/',
    'Accept-Language': 'ar-DZ,ar;q=0.9,fr-DZ;q=0.8,fr;q=0.7,en;q=0.5',
}

session = requests.Session()
retry_strategy = Retry(
    total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504],
)
adapter = HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)
session.headers.update(HEADERS)

# ==========================================
# ğŸ› ï¸ Ø§Ù„Ø¯ÙˆØ§Ù„ (Ù„Ù… ÙŠØªÙ… ØªØºÙŠÙŠØ± Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚)
# ==========================================
def convert_to_algeria_time(time_str):
    if not time_str or ":" not in time_str:
        return time_str
    try:
        is_pm = "Ù…" in time_str or "Ù…Ø³Ø§Ø¡" in time_str
        clean_time = re.sub(r'[^0-9:]', '', time_str)
        match_time = datetime.datetime.strptime(clean_time, "%H:%M")

        if is_pm and match_time.hour != 12:
            match_time = match_time.replace(hour=match_time.hour + 12)
        elif not is_pm and match_time.hour == 12:
            match_time = match_time.replace(hour=0)

        new_time = match_time + timedelta(hours=6) 
        return new_time.strftime("%H:%M")
    except:
        return time_str

def clean_text(text):
    if text:
        return text.strip().replace('\n', ' ').replace('\r', '').replace('  ', ' ')
    return None

def get_match_deep_details(match_url):
    if not match_url: return None
    full_url = match_url if match_url.startswith('http') else f"{BASE_URL}{match_url}"
    
    try:
        response = session.get(full_url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        match_id = "0"
        id_search = re.search(r'/match/(\d+)', full_url)
        if id_search:
            match_id = id_search.group(1)

        match_details = {
            "id": match_id,
            "url": full_url, 
            "info": {}, 
            "teams": {}, 
            "channels": []
        }

        team_divs = soup.find_all('div', class_=re.compile(r'(team|club)'))
        main_teams = [t for t in team_divs if t.find('img')][:2]
        
        if len(main_teams) >= 2:
            t1_name = main_teams[0].get_text(strip=True)
            t1_img = main_teams[0].find('img')['src']
            t2_name = main_teams[1].get_text(strip=True)
            t2_img = main_teams[1].find('img')['src']
            match_details["teams"]["home"] = {"name": t1_name, "logo": t1_img}
            match_details["teams"]["away"] = {"name": t2_name, "logo": t2_img}
        else:
            title_tag = soup.find('title')
            match_details["teams"]["full_title"] = title_tag.text.strip() if title_tag else "Ù…Ø¨Ø§Ø±Ø§Ø©"

        target_keys = {"Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©": "championship", "Ø§Ù„Ø¬ÙˆÙ„Ø©": "round", "Ù…Ù„Ø¹Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©": "stadium", 
                       "ÙˆÙ‚Øª Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©": "time", "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©": "date"}
        info_block = soup.find('div', class_='match-info') or soup
        for label in info_block.find_all(string=re.compile(r'Ø§Ù„Ø¨Ø·ÙˆÙ„Ø©|Ø§Ù„Ø¬ÙˆÙ„Ø©|Ù…Ù„Ø¹Ø¨|ÙˆÙ‚Øª|ØªØ§Ø±ÙŠØ®')):
            clean_lbl = clean_text(label)
            for key_ar, key_en in target_keys.items():
                if key_ar in clean_lbl:
                    parent = label.find_parent()
                    val_elem = parent.find_next_sibling() or parent.find('span', class_='value')
                    val = clean_text(val_elem.text) if val_elem else clean_text(parent.get_text().replace(key_ar, ''))
                    
                    if key_en == "time":
                        val = convert_to_algeria_time(val)
                        
                    match_details["info"][key_en] = val

        current_score = "- : -"
        match_status = ""

        s1_tag = soup.find('div', class_=re.compile(r'first-team-result')) or soup.find('span', class_=re.compile(r'first-team-result'))
        s2_tag = soup.find('div', class_=re.compile(r'second-team-result')) or soup.find('span', class_=re.compile(r'second-team-result'))
        
        if s1_tag and s2_tag:
            s1 = clean_text(s1_tag.text)
            s2 = clean_text(s2_tag.text)
            if s1.isdigit() and s2.isdigit():
                current_score = f"{s1} - {s2}"
        else:
             main_res = soup.find('div', class_='main-result')
             if main_res:
                 bs = main_res.find_all('b')
                 if len(bs) >= 2:
                     current_score = f"{clean_text(bs[0].text)} - {clean_text(bs[1].text)}"

        finished_keywords = soup.find_all(string=re.compile(r'(Ø¥Ù†ØªÙ‡Øª|Ù†Ù‡Ø§ÙŠØ©|Full Time)'))
        if finished_keywords:
             match_status = "Ø¥Ù†ØªÙ‡Øª Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©"

        if not match_status:
            live_status = soup.find('span', class_=re.compile(r'live-match-status'))
            if live_status and live_status.text.strip():
                match_status = clean_text(live_status.text)

        if not match_status:
            end_status_candidates = soup.find_all('span', class_=re.compile(r'result-status-text'))
            for status_item in end_status_candidates:
                if status_item.text.strip() and ":" not in status_item.text:
                    match_status = clean_text(status_item.text)
                    break
        
        if not match_status or ":" in match_status:
             match_status = "Ù„Ù… ØªØ¨Ø¯Ø£"

        match_details["info"]["current_score"] = current_score
        match_details["info"]["match_status"] = match_status

        section_header = soup.find(string=re.compile(r'Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù†Ø§Ù‚Ù„Ø© ÙˆØ§Ù„Ù…Ø¹Ù„Ù‚ÙŠÙ†'))
        if section_header:
            block_container = section_header.find_parent('div', class_='match-block-item')
            if block_container:
                channel_rows = block_container.find_all('div', class_='match-info-item sub')
                for row in channel_rows:
                    title_div = row.find('div', class_='title')
                    content_div = row.find('div', class_='content')
                    match_details["channels"].append({
                        "channel": clean_text(title_div.text) if title_div else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "commentator": clean_text(content_div.text) if content_div else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                    })

        if not match_details["channels"]:
            comm_single = soup.find(string=re.compile(r'^Ø§Ù„Ù…Ø¹Ù„Ù‚$'))
            ch_single = soup.find(string=re.compile(r'^Ø§Ù„Ù‚Ù†Ø§Ø©$'))
            if comm_single and ch_single:
                c_val = ch_single.find_parent().find_next_sibling()
                m_val = comm_single.find_parent().find_next_sibling()
                if c_val and m_val:
                    match_details["channels"].append({
                        "channel": clean_text(c_val.text),
                        "commentator": clean_text(m_val.text)
                    })

        return match_details

    except Exception:
        return None

def main_scraper():
    url = f"{BASE_URL}/ar/index"
    try:
        response = session.get(url, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"Error: {e}")
        return None

    links = set()
    for a in soup.find_all('a', href=re.compile(r'/match/\d+')):
        links.add(a['href'])
    
    links_list = list(links)
    total = len(links_list)
    print(f"[*] Found {total} matches.")

    final_data = []
    with ThreadPoolExecutor(max_workers=20) as executor:
        future_to_url = {executor.submit(get_match_deep_details, u): u for u in links_list}
        for future in as_completed(future_to_url):
            data = future.result()
            if data: final_data.append(data)

    return sorted(final_data, key=lambda x: x['info'].get('championship', ''))

# ==========================================
# ğŸ†• Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Cloud Firestore
# ==========================================

# âœ… Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© 'today' (ØªØ³ØªØ®Ø¯Ù… Ø¹Ù†Ø¯ Ø¨Ø¯Ø§ÙŠØ© ÙŠÙˆÙ… Ø¬Ø¯ÙŠØ¯)
def clear_old_matches():
    if not db: return
    try:
        print("ğŸ§¹ Clearing old matches from Firestore...")
        collection_ref = db.collection('today')
        # Ø¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù„Ø­Ø°ÙÙ‡Ø§
        docs = collection_ref.list_documents(page_size=100)
        deleted_count = 0
        for doc in docs:
            doc.delete()
            deleted_count += 1
        print(f"âœ… Cleared {deleted_count} old matches.")
    except Exception as e:
        print(f"âŒ Error clearing matches: {e}")

def update_firestore_db(matches_list):
    if not db:
        return False
        
    try:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Batch Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø© ÙÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ«
        batch = db.batch()
        collection_ref = db.collection('today')

        count = 0
        for match in matches_list:
            # âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ID Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© ÙƒÙ€ Document ID Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ¶Ù…Ø§Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ«
            doc_id = str(match['id']) 
            doc_ref = collection_ref.document(doc_id)
            
            # Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©
            batch.set(doc_ref, match, merge=True)
            count += 1
            
            # Firestore Limit: 500 ops per batch
            if count >= 450:
                batch.commit()
                batch = db.batch()
                count = 0
        
        if count > 0:
            batch.commit()
            
        print(f"âœ… Firestore Updated: {len(matches_list)} matches.")
        return True
    except Exception as e:
        print(f"âŒ Firestore Error: {e}")
        return False

def send_telegram_alert(message):
    if TELEGRAM_TOKEN and TELEGRAM_CHAT_ID:
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        data = {"chat_id": TELEGRAM_CHAT_ID, "text": message}
        try: session.post(url, data=data, timeout=5)
        except: pass

def monitor_matches():
    last_hash = ""
    last_update_day = datetime.date.min
    
    print(f"ğŸš€ Bot Started monitoring {BASE_URL}...")
    send_telegram_alert("ğŸš€ Bot Started on Render (Firestore).")

    while True:
        try:
            current_data = main_scraper()
            
            # âœ…âœ… ØªØ¹Ø¯ÙŠÙ„: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙˆÙ‚ÙŠØª Ø§Ù„ÙˆØ§Ø¹ÙŠ Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„ØªØ­Ø°ÙŠØ±
            # Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ ÙˆÙ‚Øª UTC Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
            utc_now = datetime.datetime.now(timezone.utc)
            
            # Ø¥Ø¶Ø§ÙØ© Ø³Ø§Ø¹Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±
            algeria_now = utc_now + timedelta(hours=1)
            current_date = algeria_now.date()
            
            if current_data:
                current_json_str = json.dumps(current_data, sort_keys=True)
                current_hash = hashlib.md5(current_json_str.encode('utf-8')).hexdigest()
                
                # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ Ø¯Ø®Ù„Ù†Ø§ ÙŠÙˆÙ…Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ (Ø­Ø³Ø¨ ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±)
                force_update = (current_date > last_update_day)
                
                if current_hash != last_hash or force_update:
                    if force_update:
                        print(f"ğŸ”„ NEW DAY ({current_date}): Clearing old data first...")
                        # âœ…âœ… Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø¬Ø¯ÙŠØ¯
                        clear_old_matches()
                        last_update_day = current_date # ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«
                    else:
                        print("ğŸ”„ Change detected! Updating...")

                    # âœ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ Cloud Firestore
                    if update_firestore_db(current_data):
                        last_hash = current_hash
                else:
                    print("ğŸ’¤ No changes.")
            
            time.sleep(CHECK_INTERVAL)

        except Exception as e:
            print(f"âš ï¸ Loop Error: {e}")
            time.sleep(60)

if __name__ == "__main__":
    keep_alive()
    monitor_matches()
